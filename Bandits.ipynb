{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project I: Multi-Armed Bandits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The multi-armed bandit problem considers a resource allocation problem in partially unknown environments. In more detail, the problem refers to the task of allocating a fixed and limited set of resources among alternative choices in a way that maximizes the expected gain. Importantly, each choice's properties are only partially known at the time of allocation and may become better understood with time and/or by allocating resources to a choice. The multi-armed bandit problem is a classic reinforcement learning problem that demonstrates the exploration-exploitation tradeoff dilemma and related concepts in a clear non-associative form. As such, it helps set the foundations of reinforcement learning theory. The multi-armed bandit problem has also applications in various domains such as clinical trials and networking. In this notebook we will experiment with a number of multiarmed bandit algorithms by altering the abstract bandit algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![robot](img/robot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: General Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: The General Bandit Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![algorithm](img/abstract_algorithm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we declare a general bandit algorithm that accepts various initialization, action selection and update strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bandit(inputs):\n",
    "    \"\"\"\n",
    "        An abstract k-arm bandit algorithm.\n",
    "\n",
    "        Args:\n",
    "            inputs: A dictionary containing the number of actions \"k\", the number of iterations \"horizon\",\n",
    "                    the true unknown rewards \"q_star\", the initialization strategy function \"initialization\",\n",
    "                    the action selection strategy function \"action_selection\", the update rule function \"update rule\"\n",
    "                    and any other parameters required.\n",
    "\n",
    "        Returns:\n",
    "            q_estimates: list - the estimates of the q values.\n",
    "            total_reward: float - the cummulative reward\n",
    "            mean_rewards: list - the consecutive mean rewards\n",
    "            regret: list - the true maximum regret\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    #Initializing rewards and regret (used mainly for evaluation purposes)\n",
    "    total_reward = 0\n",
    "    mean_rewards = np.zeros(inputs[\"horizon\"])\n",
    "    regret = np.zeros(inputs[\"horizon\"])\n",
    "    \n",
    "    #Initialize an action selection counter (used mainly for the true average update rule)\n",
    "    action_select_counter = np.zeros(k)\n",
    "    \n",
    "    ### INITIALIZE THE ESTIMATES\n",
    "    q_estimates = inputs[\"initialization\"](inputs[\"k\"], inputs[\"q_star\"])\n",
    "    \n",
    "    for t in range(1,inputs[\"horizon\"]+1): # loop over the horizon\n",
    "        \n",
    "        ### ACTION SELECTION\n",
    "        action = inputs[\"action_selection\"](q_estimates, inputs, t, action_select_counter)    \n",
    "        # Update action counter\n",
    "        action_select_counter[action] += 1\n",
    "    \n",
    "        ## BANDIT CALL\n",
    "        reward = run_bandit(action, q_star) \n",
    "        \n",
    "        #Calculate reward, mean reward and regret for evaluatuion purposes\n",
    "        total_reward += reward      \n",
    "        mean_rewards[t-1] = total_reward/t\n",
    "        regret[t-1] = np.max(q_star) - reward # We use the true maximum to calculate the regret :)\n",
    "        \n",
    "        ## UPDATE ACTION-VALUE ESTIMATES\n",
    "        q_estimates = inputs[\"update_rule\"](q_estimates, action, reward, inputs, action_select_counter, mean_rewards[t-1])\n",
    "        \n",
    "    return q_estimates, total_reward, mean_rewards, regret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_bandit(action, q_star):\n",
    "    \"\"\" \n",
    "        Returns the reward of an action assuming that it follows a normal distribution with variance 1.\n",
    "        The mean values are extracted from q_star \n",
    "    \"\"\"\n",
    "    return rng2.normal(q_star[action], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Initialization strategies\n",
    "Using the q* values, we create initialization methods that implement zero, pessimistic, average, optimistic and random estimate initializations for the bandit algorithm:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimate initialization with zeroes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ZeroInitialization(k, q_star):\n",
    "    \"\"\"Zero initialization\"\"\"\n",
    "    return np.zeros(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pessimistic estimate initialization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MinInitialization(k, q_star):\n",
    "    \"\"\"Min initialization\"\"\"\n",
    "    return np.ones(k) * np.min(q_star)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean estimate initialization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MeanInitialization(k, q_star):\n",
    "    \"\"\"Mean initialization\"\"\"\n",
    "    return np.ones(k) * np.mean(q_star)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimistic estimate initialization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MaxInitialization(k, q_star):\n",
    "    \"\"\"Max initialization\"\"\"\n",
    "    return np.ones(k) * np.max(q_star)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimate initialization with random numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RandomInitialization(k, q_star):\n",
    "    \"\"\"Random initialization\"\"\"\n",
    "    return rng3.normal(0, 1, k)  #let's assume we know the distributions and take nice random initial values :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Action selection strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random action selection: \n",
    "This strategy selects an action completely at random without taking into account the learned estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_action_selection(estimates, bandit_inputs, t, action_select_counter):\n",
    "    \"\"\"Choose an action randomly (uniform)\"\"\"\n",
    "    return rng2.randint(inputs[\"k\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Greedy action selection:\n",
    "This action selection strategy always selects the best possible action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_action_selection(estimates, inputs, t, action_select_counter):\n",
    "    \"\"\" Choose an action using the greedy action selection\"\"\"\n",
    "    return np.argmax(estimates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ε-Greedy action selection:\n",
    "The implementation of the e-Greedy algorithm for the multi-armed bandit problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def e_greedy_action_selection(estimates, inputs, t, action_select_counter):\n",
    "    \"\"\" Choose an action using the e-greedy action selection\"\"\"\n",
    "    # Generate a random number\n",
    "    p = rng2.rand()\n",
    "    \n",
    "    # E-Greedy action selection\n",
    "    if p < inputs[\"epsilon\"]:\n",
    "        # Randomly select an action\n",
    "        return rng2.choice(inputs[\"k\"])\n",
    "    else:\n",
    "        # Take greedy action\n",
    "        return np.argmax(estimates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ε-Greedy with epsion decay action selection:\n",
    "The implementation of the e-Greedy algorithm with epsilon decay for the multi-armed bandit problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def e_decay_greedy_action_selection(estimates, inputs, t, action_select_counter):\n",
    "    \"\"\" Choose an action using the e-greedy with epsilon decay action selection\"\"\"\n",
    "    # Generate a random number\n",
    "    p = rng2.rand()\n",
    "\n",
    "    # E-Greedy action selection\n",
    "    if p < inputs[\"epsilon\"]*np.exp(-inputs[\"kappa\"]*t):\n",
    "        # Randomly select an action\n",
    "        action = rng2.choice(inputs[\"k\"])\n",
    "    else:\n",
    "        # Take greedy action\n",
    "        action = np.argmax(estimates)\n",
    "            \n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upper Confidence Bound action selection:\n",
    "The implementation of the UCB algorithm for the multi armed bandit problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ucb_action_selection(estimates, inputs, t, action_select_counter):\n",
    "    \"\"\" Choose an action using the UCB action selection\"\"\"\n",
    "    t = np.sum(action_select_counter) + 1\n",
    "    # Select action according to UCB Criteria\n",
    "    return np.argmax(estimates + inputs[\"c\"] * np.sqrt(np.log(t) / (action_select_counter+1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SoftMax action selection:\n",
    "The implementation of the softmax bandit algorithm for the multi-armed bandit problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_action_selection(estimates, inputs, t, action_select_counter):\n",
    "    \"\"\" Choose an action using the Softmax action selection\"\"\"\n",
    "    # Softmax action selection\n",
    "    action = rng3.choice(inputs[\"k\"], 1, p=np.exp(estimates/inputs[\"tau\"]) / np.sum(np.exp(estimates/inputs[\"tau\"])))\n",
    "    return action[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Update strategies "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### True average update rule:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_rule(estimates, action, reward, inputs, action_select_counter, mean_reward):\n",
    "    \"\"\" Return the estimates using the true average update rule\"\"\"\n",
    "    # Update the estimates\n",
    "    estimates[action] = estimates[action] + (reward - estimates[action]) / action_select_counter[action]\n",
    "    return estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constant learning rate update rule:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_rule_constant_lr(estimates, action, reward, inputs, action_select_counter, mean_reward):\n",
    "    # Update the estimates\n",
    "    \"\"\" Return the estimates using the constant learning rate update rule\"\"\"\n",
    "    estimates[action] = estimates[action] + (reward - estimates[action]) / (action_select_counter[action]*inputs[\"a\"])\n",
    "    return estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about decaying learning rate?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: The Case Study Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we declare the number of trials and arms of the multi-armed bandit problem. We also select a gaussian probability distribution to generate the true values of each arm and to simulate the randomness of the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of actions\n",
    "k = 10\n",
    "# Number of iterations\n",
    "horizon = 5000\n",
    "#Number of trials\n",
    "trials=100\n",
    "\n",
    "# Initialization of the true action values (q*) according to a gaussian distribution\n",
    "\n",
    "q_star = [-0.70318731, -0.49028236, -0.32181433, -1.75507872,  0.20666447, -2.01126457, -0.55725071,  0.33721701,  1.54883597, -1.37073656]\n",
    "#rng1 = np.random.RandomState(1337) #https://en.wikipedia.org/wiki/Leet\n",
    "#q_star = rng1.normal(0, 1, k)\n",
    "#print(q_star)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Experimental procedure "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the experimental procedure followed for all experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def procedure(inputs, N):\n",
    "    \"\"\" \n",
    "        Does \"trials\" amount of runs of the algorithm specified by the \"inputs\" dictionary. \n",
    "    \n",
    "        Args:\n",
    "            inputs: A dictionary containing the number of actions \"k\", the number of iterations \"horizon\",\n",
    "                    the true unknown rewards \"q_star\", the initialization strategy function \"initialization\",\n",
    "                    the action selection strategy function \"action_selection\", the update rule function \"update rule\"\n",
    "                    and any other parameters required.\n",
    "            trials: The amount of trial runs to execute so we can get the average performance of the algorithm\n",
    "\n",
    "        Returns:\n",
    "            average_q_estimates: list - the average estimates of the q values.\n",
    "            average_total_reward: float - the average cummulative reward\n",
    "            average_mean_rewards: list - the average consecutive mean rewards\n",
    "            average_regret: list - the average true maximum regret\n",
    "    \"\"\"\n",
    "    #Initialize the evaluation parameters\n",
    "    estimates = np.zeros(k)\n",
    "    total_reward = 0\n",
    "    mean_rewards = np.zeros(N)\n",
    "    regret = np.zeros(N)\n",
    "\n",
    "    #Run the n trials of the experiment\n",
    "    for i in range(trials):\n",
    "        i_estimates, i_total_reward, i_mean_rewards, i_regret = bandit(inputs)\n",
    "        estimates += i_estimates\n",
    "        total_reward += i_total_reward\n",
    "        mean_rewards += i_mean_rewards\n",
    "        regret += i_regret\n",
    "        \n",
    "    return estimates/trials, total_reward/trials, mean_rewards/trials, regret/trials #return the average results for all trials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Plotting/printing:\n",
    "After each experiment we print two plots to evaluate the performance of the algorithm. We print the loss that we incur due to time/rounds spent due to the learning, or else the regret, and the expected reward of the algorithm across the rounds of each experiment.\n",
    "\n",
    "Definition of the print and plot of the results (we use this throughout the rest of the notebook):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bandits(N, estimates, total_reward, mean_rewards, regret, q_star):\n",
    "    \"\"\" Plots using matplotlib the regret curve and the average reward curve\"\"\"\n",
    "    print(\"True q values:{}\".format(q_star))\n",
    "    print(\"Learned Estimates: {}\".format(estimates))\n",
    "    print(\"\")\n",
    "    print(\"Euclidean distance from q_star vector: {}\".format(np.linalg.norm(q_star-estimates)))\n",
    "    print(\"Total Reward: {}\".format(total_reward))\n",
    "    print(\"Mean Reward: {}\".format(mean_rewards[-1]))\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(np.linspace(0,N-1,N), np.cumsum(regret), 'b-')\n",
    "    ax.set_xlabel(\"Time\")\n",
    "    ax.set_ylabel(\"Speed\")\n",
    "    ax.grid()\n",
    "    ax.set(xlabel='Time Steps', ylabel='Total Regret',\n",
    "           title='Regret Curve')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(np.linspace(0,N-1,N), mean_rewards, 'b-')\n",
    "    ax.set_xlabel(\"Time\")\n",
    "    ax.set_ylabel(\"Speed\")\n",
    "    ax.grid()\n",
    "    ax.set(xlabel='Time Steps', ylabel='Average Reward',\n",
    "           title='Average Reward Curve')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition of the comp_plot, that plots the comparative plot of the various initializations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comp_plot(regret, pes_regret, avg_regret, opt_regret, rnd_regret, \\\n",
    "              mean_rewards, pes_mean_rewards, avg_mean_rewards, opt_mean_rewards, rnd_mean_rewards):\n",
    "    \"\"\" Plots using matplotlib a comparison of the the regret curves and the average reward curves\"\"\"\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(np.linspace(0,horizon-1,horizon), np.cumsum(regret), 'b-')\n",
    "    ax.plot(np.linspace(0,horizon-1,horizon), np.cumsum(opt_regret), 'g-')\n",
    "    ax.plot(np.linspace(0,horizon-1,horizon), np.cumsum(avg_regret), 'r-')\n",
    "    ax.plot(np.linspace(0,horizon-1,horizon), np.cumsum(pes_regret), 'c-')\n",
    "    ax.plot(np.linspace(0,horizon-1,horizon), np.cumsum(rnd_regret), 'y-')\n",
    "    ax.set_xlabel(\"Time\")\n",
    "    ax.set_ylabel(\"Speed\")\n",
    "    ax.grid()\n",
    "    ax.set(xlabel='Time Steps', ylabel='Total Regret',\n",
    "           title='Regret Curve')\n",
    "    plt.legend(['Zero','Optimistic','Mean','Pesimistic','Random'])\n",
    "    plt.show()\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(np.linspace(0,horizon-1,horizon), mean_rewards, 'b-')\n",
    "    ax.plot(np.linspace(0,horizon-1,horizon), opt_mean_rewards, 'g-')\n",
    "    ax.plot(np.linspace(0,horizon-1,horizon), avg_mean_rewards, 'r-')\n",
    "    ax.plot(np.linspace(0,horizon-1,horizon), pes_mean_rewards, 'c-')\n",
    "    ax.plot(np.linspace(0,horizon-1,horizon), rnd_mean_rewards, 'y-')\n",
    "    ax.set_xlabel(\"Time\")\n",
    "    ax.set_ylabel(\"Speed\")\n",
    "    ax.grid()\n",
    "    ax.set(xlabel='Time Steps', ylabel='Average Reward',\n",
    "           title='Average Reward Curve')\n",
    "    plt.legend(['Zero','Optimistic','Mean','Pesimistic','Random'])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4. The bandit algorithm with random action selection and true average update (Experiment I)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Initializing with zero values:\n",
    "What do you think the expected reward is? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Seed our random engines:\n",
    "rng2 = np.random.RandomState(1337)\n",
    "rng3 = np.random.RandomState(1337)\n",
    "\n",
    "#Initialize the experiment:\n",
    "inputs = {\"k\"                 : k, # Number of actions\n",
    "          \"horizon\"           : horizon, # Number of iterations\n",
    "          \"q_star\"            : q_star, # The true unknown rewards\n",
    "          \"initialization\"    : ZeroInitialization, # Initialization strategy\n",
    "          \"action_selection\"  : random_action_selection, # Action selection strategy\n",
    "          \"update_rule\"       : update_rule # The update rule\n",
    "         }\n",
    "\n",
    "#Run the experiment and plot:\n",
    "estimates_rand, total_reward_rand, mean_rewards_rand, regret_rand = procedure(inputs,horizon)\n",
    "plot_bandits(horizon, estimates_rand, total_reward_rand, mean_rewards_rand, regret_rand, q_star)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Initializing with Pessimistic:\n",
    "What do you think the expected reward is? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seed our random engines:\n",
    "rng2 = np.random.RandomState(1337)\n",
    "rng3 = np.random.RandomState(1337)\n",
    "\n",
    "#Initialize the experiment:\n",
    "inputs = {\"k\"                 : k, # Number of actions\n",
    "          \"horizon\"           : horizon, # Number of iterations\n",
    "          \"q_star\"            : q_star, # The true unknown rewards\n",
    "          \"initialization\"    : MinInitialization, # Initialization strategy\n",
    "          \"action_selection\"  : random_action_selection, # Action selection strategy\n",
    "          \"update_rule\"       : update_rule # The update rule\n",
    "         }\n",
    "\n",
    "#Run the experiment and plot:\n",
    "min_estimates_rand, min_total_reward_rand, min_mean_rewards_rand, min_regret_rand = procedure(inputs,horizon)\n",
    "plot_bandits(horizon, min_estimates_rand, min_total_reward_rand, min_mean_rewards_rand, min_regret_rand, q_star)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Initializing with Mean:\n",
    "What do you think the expected reward is?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Seed our random engines:\n",
    "rng2 = np.random.RandomState(1337)\n",
    "rng3 = np.random.RandomState(1337)\n",
    "\n",
    "#Initialize the experiment:\n",
    "inputs = {\"k\"                 : k, # Number of actions\n",
    "          \"horizon\"           : horizon, # Number of iterations\n",
    "          \"q_star\"            : q_star, # The true unknown rewards\n",
    "          \"initialization\"    : MeanInitialization, # Initialization strategy\n",
    "          \"action_selection\"  : random_action_selection, # Action selection strategy\n",
    "          \"update_rule\"       : update_rule # The update rule\n",
    "         }\n",
    "\n",
    "#Run the experiment and plot:\n",
    "avg_estimates_rand, avg_total_reward_rand, avg_mean_rewards_rand, avg_regret_rand = procedure(inputs,horizon)\n",
    "plot_bandits(horizon, avg_estimates_rand, avg_total_reward_rand, avg_mean_rewards_rand, avg_regret_rand, q_star)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4. Initializing with Optimistic:\n",
    "What do you think the expected reward is?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seed our random engines:\n",
    "rng2 = np.random.RandomState(1337)\n",
    "rng3 = np.random.RandomState(1337)\n",
    "\n",
    "#Initialize the experiment:\n",
    "inputs = {\"k\"                 : k, # Number of actions\n",
    "          \"horizon\"           : horizon, # Number of iterations\n",
    "          \"q_star\"            : q_star, # The true unknown rewards\n",
    "          \"initialization\"    : MaxInitialization, # Initialization strategy\n",
    "          \"action_selection\"  : random_action_selection, # Action selection strategy\n",
    "          \"update_rule\"       : update_rule # The update rule\n",
    "         }\n",
    "\n",
    "#Run the experiment and plot:\n",
    "max_estimates_rand, max_total_reward_rand, max_mean_rewards_rand, max_regret_rand = procedure(inputs,horizon)\n",
    "plot_bandits(horizon, max_estimates_rand, max_total_reward_rand, max_mean_rewards_rand, max_regret_rand, q_star)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5. Initializing with Random\n",
    "What do you think the expected reward is?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seed our random engines:\n",
    "rng2 = np.random.RandomState(1337)\n",
    "rng3 = np.random.RandomState(1337)\n",
    "\n",
    "#Initialize the experiment:\n",
    "inputs = {\"k\"                 : k, # Number of actions\n",
    "          \"horizon\"           : horizon, # Number of iterations\n",
    "          \"q_star\"            : q_star, # The true unknown rewards\n",
    "          \"initialization\"    : RandomInitialization, # Initialization strategy\n",
    "          \"action_selection\"  : random_action_selection, # Action selection strategy\n",
    "          \"update_rule\"       : update_rule # The update rule\n",
    "         }\n",
    "\n",
    "#Run the experiment and plot:\n",
    "rnd_estimates_rand, rnd_total_reward_rand, rnd_mean_rewards_rand, rnd_regret_rand = procedure(inputs,horizon)\n",
    "plot_bandits(horizon, rnd_estimates_rand, rnd_total_reward_rand, rnd_mean_rewards_rand, rnd_regret_rand, q_star)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6. Compare all:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see a comparison between the different initialization strategies for the random action selection strategy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_plot(regret_rand, min_regret_rand, avg_regret_rand, max_regret_rand, rnd_regret_rand, \\\n",
    "          mean_rewards_rand, min_mean_rewards_rand, avg_mean_rewards_rand, max_mean_rewards_rand, rnd_mean_rewards_rand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: The bandit algorithm with greedy action selection and true average update (Experiment II)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Initializing with zero values:\n",
    "What do you think the expected reward is? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Seed our random engines:\n",
    "rng2 = np.random.RandomState(1337)\n",
    "rng3 = np.random.RandomState(1337)\n",
    "\n",
    "#Initialize the experiment:\n",
    "inputs = {\"k\"                 : k, # Number of actions\n",
    "          \"horizon\"           : horizon, # Number of iterations\n",
    "          \"q_star\"            : q_star, # The true unknown rewards\n",
    "          \"initialization\"    : ZeroInitialization, # Initialization strategy\n",
    "          \"action_selection\"  : greedy_action_selection, # Action selection strategy\n",
    "          \"update_rule\"       : update_rule # The update rule\n",
    "         }\n",
    "\n",
    "#Run the experiment and plot:\n",
    "estimates_greedy, total_reward_greedy, mean_rewards_greedy, regret_greedy = procedure(inputs,horizon)\n",
    "plot_bandits(horizon, estimates_greedy, total_reward_greedy, mean_rewards_greedy, regret_greedy, q_star)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Initializing with Pessimistic:\n",
    "What do you think the expected reward is? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seed our random engines:\n",
    "rng2 = np.random.RandomState(1337)\n",
    "rng3 = np.random.RandomState(1337)\n",
    "\n",
    "#Initialize the experiment:\n",
    "inputs = {\"k\"                 : k, # Number of actions\n",
    "          \"horizon\"           : horizon, # Number of iterations\n",
    "          \"q_star\"            : q_star, # The true unknown rewards\n",
    "          \"initialization\"    : MinInitialization, # Initialization strategy\n",
    "          \"action_selection\"  : greedy_action_selection, # Action selection strategy\n",
    "          \"update_rule\"       : update_rule # The update rule\n",
    "         }\n",
    "\n",
    "#Run the experiment and plot:\n",
    "min_estimates_greedy, min_total_reward_greedy, min_mean_rewards_greedy, min_regret_greedy = procedure(inputs,horizon)\n",
    "plot_bandits(horizon, min_estimates_greedy, min_total_reward_greedy, min_mean_rewards_greedy, min_regret_greedy, q_star)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3. Initializing with Mean:\n",
    "What do you think the expected reward is?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Seed our random engines:\n",
    "rng2 = np.random.RandomState(1337)\n",
    "rng3 = np.random.RandomState(1337)\n",
    "\n",
    "#Initialize the experiment:\n",
    "inputs = {\"k\"                 : k, # Number of actions\n",
    "          \"horizon\"           : horizon, # Number of iterations\n",
    "          \"q_star\"            : q_star, # The true unknown rewards\n",
    "          \"initialization\"    : MeanInitialization, # Initialization strategy\n",
    "          \"action_selection\"  : greedy_action_selection, # Action selection strategy\n",
    "          \"update_rule\"       : update_rule # The update rule\n",
    "         }\n",
    "\n",
    "#Run the experiment and plot:\n",
    "avg_estimates_greedy, avg_total_reward_greedy, avg_mean_rewards_greedy, avg_regret_greedy = procedure(inputs,horizon)\n",
    "plot_bandits(horizon, avg_estimates_greedy, avg_total_reward_greedy, avg_mean_rewards_greedy, avg_regret_greedy, q_star)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4. Initializing with Optimistic:\n",
    "What do you think the expected reward is?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seed our random engines:\n",
    "rng2 = np.random.RandomState(1337)\n",
    "rng3 = np.random.RandomState(1337)\n",
    "\n",
    "#Initialize the experiment:\n",
    "inputs = {\"k\"                 : k, # Number of actions\n",
    "          \"horizon\"           : horizon, # Number of iterations\n",
    "          \"q_star\"            : q_star, # The true unknown rewards\n",
    "          \"initialization\"    : MaxInitialization, # Initialization strategy\n",
    "          \"action_selection\"  : greedy_action_selection, # Action selection strategy\n",
    "          \"update_rule\"       : update_rule # The update rule\n",
    "         }\n",
    "\n",
    "#Run the experiment and plot:\n",
    "max_estimates_greedy, max_total_reward_greedy, max_mean_rewards_greedy, max_regret_greedy = procedure(inputs,horizon)\n",
    "plot_bandits(horizon, max_estimates_greedy, max_total_reward_greedy, max_mean_rewards_greedy, max_regret_greedy, q_star)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5. Initializing with Random\n",
    "What do you think the expected reward is?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seed our random engines:\n",
    "rng2 = np.random.RandomState(1337)\n",
    "rng3 = np.random.RandomState(1337)\n",
    "\n",
    "#Initialize the experiment:\n",
    "inputs = {\"k\"                 : k, # Number of actions\n",
    "          \"horizon\"           : horizon, # Number of iterations\n",
    "          \"q_star\"            : q_star, # The true unknown rewards\n",
    "          \"initialization\"    : RandomInitialization, # Initialization strategy\n",
    "          \"action_selection\"  : greedy_action_selection, # Action selection strategy\n",
    "          \"update_rule\"       : update_rule # The update rule\n",
    "         }\n",
    "\n",
    "#Run the experiment and plot:\n",
    "rnd_estimates_greedy, rnd_total_reward_greedy, rnd_mean_rewards_greedy, rnd_regret_greedy = procedure(inputs,horizon)\n",
    "plot_bandits(horizon, rnd_estimates_greedy, rnd_total_reward_greedy, rnd_mean_rewards_greedy, rnd_regret_greedy, q_star)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6 Compare all:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see a comparison between the different initialization strategies for the random action selection strategy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_plot(regret_greedy, min_regret_greedy, avg_regret_greedy, max_regret_greedy, rnd_regret_greedy, \\\n",
    "          mean_rewards_greedy, min_mean_rewards_greedy, avg_mean_rewards_greedy, max_mean_rewards_greedy, \\\n",
    "          rnd_mean_rewards_greedy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: The bandit algorithm with ε-Greedy action selection and true average update (Experiment III)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <div style=\"text-align: right\"><span style=\"color:red\">Maybe test out different ε values? :)</span></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1. Initializing with zero values:\n",
    "What do you think the expected reward is? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Seed our random engines:\n",
    "rng2 = np.random.RandomState(1337)\n",
    "rng3 = np.random.RandomState(1337)\n",
    "\n",
    "#Initialize the experiment:\n",
    "inputs = {\"k\"                 : k, # Number of actions\n",
    "          \"horizon\"           : horizon, # Number of iterations\n",
    "          \"q_star\"            : q_star, # The true unknown rewards\n",
    "          \"initialization\"    : ZeroInitialization, # Initialization strategy\n",
    "          \"action_selection\"  : e_greedy_action_selection, # Action selection strategy\n",
    "          \"update_rule\"       : update_rule, # The update rule\n",
    "          \"epsilon\"           : 0.15\n",
    "         }\n",
    "\n",
    "#Run the experiment and plot:\n",
    "estimates_e_greedy, total_reward_e_greedy, mean_rewards_e_greedy, regret_e_greedy = procedure(inputs,horizon)\n",
    "plot_bandits(horizon, estimates_e_greedy, total_reward_e_greedy, mean_rewards_e_greedy, regret_e_greedy, q_star)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2. Initializing with Pessimistic:\n",
    "What do you think the expected reward is? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seed our random engines:\n",
    "rng2 = np.random.RandomState(1337)\n",
    "rng3 = np.random.RandomState(1337)\n",
    "\n",
    "#Initialize the experiment:\n",
    "inputs = {\"k\"                 : k, # Number of actions\n",
    "          \"horizon\"           : horizon, # Number of iterations\n",
    "          \"q_star\"            : q_star, # The true unknown rewards\n",
    "          \"initialization\"    : MinInitialization, # Initialization strategy\n",
    "          \"action_selection\"  : e_greedy_action_selection, # Action selection strategy\n",
    "          \"update_rule\"       : update_rule, # The update rule\n",
    "          \"epsilon\"           : 0.15\n",
    "         }\n",
    "\n",
    "#Run the experiment and plot:\n",
    "min_estimates_e_greedy, min_total_reward_e_greedy, min_mean_rewards_e_greedy, min_regret_e_greedy = procedure(inputs,horizon)\n",
    "plot_bandits(horizon, min_estimates_e_greedy, min_total_reward_e_greedy, min_mean_rewards_e_greedy, min_regret_e_greedy, q_star)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3. Initializing with Mean:\n",
    "What do you think the expected reward is?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Seed our random engines:\n",
    "rng2 = np.random.RandomState(1337)\n",
    "rng3 = np.random.RandomState(1337)\n",
    "\n",
    "#Initialize the experiment:\n",
    "inputs = {\"k\"                 : k, # Number of actions\n",
    "          \"horizon\"           : horizon, # Number of iterations\n",
    "          \"q_star\"            : q_star, # The true unknown rewards\n",
    "          \"initialization\"    : MeanInitialization, # Initialization strategy\n",
    "          \"action_selection\"  : e_greedy_action_selection, # Action selection strategy\n",
    "          \"update_rule\"       : update_rule, # The update rule\n",
    "          \"epsilon\"           : 0.15\n",
    "         }\n",
    "\n",
    "#Run the experiment and plot:\n",
    "avg_estimates_e_greedy, avg_total_reward_e_greedy, avg_mean_rewards_e_greedy, avg_regret_e_greedy = procedure(inputs,horizon)\n",
    "plot_bandits(horizon, avg_estimates_e_greedy, avg_total_reward_e_greedy, avg_mean_rewards_e_greedy, avg_regret_e_greedy, q_star)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4. Initializing with Optimistic:\n",
    "What do you think the expected reward is?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seed our random engines:\n",
    "rng2 = np.random.RandomState(1337)\n",
    "rng3 = np.random.RandomState(1337)\n",
    "\n",
    "#Initialize the experiment:\n",
    "inputs = {\"k\"                 : k, # Number of actions\n",
    "          \"horizon\"           : horizon, # Number of iterations\n",
    "          \"q_star\"            : q_star, # The true unknown rewards\n",
    "          \"initialization\"    : MaxInitialization, # Initialization strategy\n",
    "          \"action_selection\"  : e_greedy_action_selection, # Action selection strategy\n",
    "          \"update_rule\"       : update_rule, # The update rule\n",
    "          \"epsilon\"           : 0.15\n",
    "         }\n",
    "\n",
    "#Run the experiment and plot:\n",
    "max_estimates_e_greedy, max_total_reward_e_greedy, max_mean_rewards_e_greedy, max_regret_e_greedy = procedure(inputs,horizon)\n",
    "plot_bandits(horizon, max_estimates_e_greedy, max_total_reward_e_greedy, max_mean_rewards_e_greedy, max_regret_e_greedy, q_star)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5. Initializing with Random\n",
    "What do you think the expected reward is?\n",
    "(we will use the average of 100 random initializations, so that the results are significant):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seed our random engines:\n",
    "rng2 = np.random.RandomState(1337)\n",
    "rng3 = np.random.RandomState(1337)\n",
    "\n",
    "#Initialize the experiment:\n",
    "inputs = {\"k\"                 : k, # Number of actions\n",
    "          \"horizon\"           : horizon, # Number of iterations\n",
    "          \"q_star\"            : q_star, # The true unknown rewards\n",
    "          \"initialization\"    : RandomInitialization, # Initialization strategy\n",
    "          \"action_selection\"  : e_greedy_action_selection, # Action selection strategy\n",
    "          \"update_rule\"       : update_rule, # The update rule\n",
    "          \"epsilon\"           : 0.15\n",
    "         }\n",
    "\n",
    "#Run the experiment and plot:\n",
    "rnd_estimates_e_greedy, rnd_total_reward_e_greedy, rnd_mean_rewards_e_greedy, rnd_regret_e_greedy = procedure(inputs,horizon)\n",
    "plot_bandits(horizon, rnd_estimates_e_greedy, rnd_total_reward_e_greedy, rnd_mean_rewards_e_greedy, rnd_regret_e_greedy, q_star)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.6. Compare all:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see a comparison between the different initialization strategies for the e-greedy action selection strategy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_plot(regret_e_greedy, min_regret_e_greedy, avg_regret_e_greedy, max_regret_e_greedy, rnd_regret_e_greedy, \\\n",
    "          mean_rewards_e_greedy, min_mean_rewards_e_greedy, avg_mean_rewards_e_greedy, max_mean_rewards_e_greedy, \\\n",
    "          rnd_mean_rewards_e_greedy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7: The bandit algorithm with ε-Greedy with epsilon decay action selection and true average update (Experiment IV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <div style=\"text-align: right\"><span style=\"color:red\">Maybe test out different ε and kappa values? (Especially the limit cases 0, 1):)</span></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1. Initializing with zero values:\n",
    "What do you think the expected reward is? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Seed our random engines:\n",
    "rng2 = np.random.RandomState(1337)\n",
    "rng3 = np.random.RandomState(1337)\n",
    "\n",
    "#Initialize the experiment:\n",
    "inputs = {\"k\"                 : k, # Number of actions\n",
    "          \"horizon\"           : horizon, # Number of iterations\n",
    "          \"q_star\"            : q_star, # The true unknown rewards\n",
    "          \"initialization\"    : ZeroInitialization, # Initialization strategy\n",
    "          \"action_selection\"  : e_decay_greedy_action_selection, # Action selection strategy\n",
    "          \"update_rule\"       : update_rule, # The update rule\n",
    "          \"epsilon\"           : 0.1,# The epsilon parameter. (try with 0.01 as well--will it get stuck to lockal minima?)\n",
    "          \"kappa\"             : 0.001, # Decay coefficient\n",
    "         }\n",
    "\n",
    "#Run the experiment and plot:\n",
    "estimates_e_decay_greedy, total_reward_e_decay_greedy, mean_rewards_e_decay_greedy, regret_e_decay_greedy = procedure(inputs,horizon)\n",
    "plot_bandits(horizon, estimates_e_decay_greedy, total_reward_e_decay_greedy, mean_rewards_e_decay_greedy, regret_e_decay_greedy, q_star)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2. Initializing with Pessimistic:\n",
    "What do you think the expected reward is? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seed our random engines:\n",
    "rng2 = np.random.RandomState(1337)\n",
    "rng3 = np.random.RandomState(1337)\n",
    "\n",
    "#Initialize the experiment:\n",
    "inputs = {\"k\"                 : k, # Number of actions\n",
    "          \"horizon\"           : horizon, # Number of iterations\n",
    "          \"q_star\"            : q_star, # The true unknown rewards\n",
    "          \"initialization\"    : MinInitialization, # Initialization strategy\n",
    "          \"action_selection\"  : e_decay_greedy_action_selection, # Action selection strategy\n",
    "          \"update_rule\"       : update_rule, # The update rule\n",
    "          \"epsilon\"           : 0.1,# The epsilon parameter. (try with 0.01 as well--will it get stuck to lockal minima?)\n",
    "          \"kappa\"             : 0.001, # Decay coefficient\n",
    "         }\n",
    "\n",
    "#Run the experiment and plot:\n",
    "min_estimates_e_decay_greedy, min_total_reward_e_decay_greedy, min_mean_rewards_e_decay_greedy, min_regret_e_decay_greedy = procedure(inputs,horizon)\n",
    "plot_bandits(horizon, min_estimates_e_decay_greedy, min_total_reward_e_decay_greedy, min_mean_rewards_e_decay_greedy, min_regret_e_decay_greedy, q_star)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3. Initializing with Mean:\n",
    "What do you think the expected reward is?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Seed our random engines:\n",
    "rng2 = np.random.RandomState(1337)\n",
    "rng3 = np.random.RandomState(1337)\n",
    "\n",
    "#Initialize the experiment:\n",
    "inputs = {\"k\"                 : k, # Number of actions\n",
    "          \"horizon\"           : horizon, # Number of iterations\n",
    "          \"q_star\"            : q_star, # The true unknown rewards\n",
    "          \"initialization\"    : MeanInitialization, # Initialization strategy\n",
    "          \"action_selection\"  : e_decay_greedy_action_selection, # Action selection strategy\n",
    "          \"update_rule\"       : update_rule, # The update rule\n",
    "          \"epsilon\"           : 0.1,# The epsilon parameter. (try with 0.01 as well--will it get stuck to lockal minima?)\n",
    "          \"kappa\"             : 0.001, # Decay coefficient\n",
    "         }\n",
    "\n",
    "#Run the experiment and plot:\n",
    "avg_estimates_e_decay_greedy, avg_total_reward_e_decay_greedy, avg_mean_rewards_e_decay_greedy, avg_regret_e_decay_greedy = procedure(inputs,horizon)\n",
    "plot_bandits(horizon, avg_estimates_e_decay_greedy, avg_total_reward_e_decay_greedy, avg_mean_rewards_e_decay_greedy, avg_regret_e_decay_greedy, q_star)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4. Initializing with Optimistic:\n",
    "What do you think the expected reward is?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seed our random engines:\n",
    "rng2 = np.random.RandomState(1337)\n",
    "rng3 = np.random.RandomState(1337)\n",
    "\n",
    "#Initialize the experiment:\n",
    "inputs = {\"k\"                 : k, # Number of actions\n",
    "          \"horizon\"           : horizon, # Number of iterations\n",
    "          \"q_star\"            : q_star, # The true unknown rewards\n",
    "          \"initialization\"    : MaxInitialization, # Initialization strategy\n",
    "          \"action_selection\"  : e_decay_greedy_action_selection, # Action selection strategy\n",
    "          \"update_rule\"       : update_rule, # The update rule\n",
    "          \"epsilon\"           : 0.1,# The epsilon parameter. (try with 0.01 as well--will it get stuck to lockal minima?)\n",
    "          \"kappa\"             : 0.001, # Decay coefficient\n",
    "         }\n",
    "\n",
    "#Run the experiment and plot:\n",
    "max_estimates_e_decay_greedy, max_total_reward_e_decay_greedy, max_mean_rewards_e_decay_greedy, max_regret_e_decay_greedy = procedure(inputs,horizon)\n",
    "plot_bandits(horizon, max_estimates_e_decay_greedy, max_total_reward_e_decay_greedy, max_mean_rewards_e_decay_greedy, max_regret_e_decay_greedy, q_star)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.5. Initializing with Random\n",
    "What do you think the expected reward is?\n",
    "(we will use the average of 100 random initializations, so that the results are significant):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seed our random engines:\n",
    "rng2 = np.random.RandomState(1337)\n",
    "rng3 = np.random.RandomState(1337)\n",
    "\n",
    "#Initialize the experiment:\n",
    "inputs = {\"k\"                 : k, # Number of actions\n",
    "          \"horizon\"           : horizon, # Number of iterations\n",
    "          \"q_star\"            : q_star, # The true unknown rewards\n",
    "          \"initialization\"    : RandomInitialization, # Initialization strategy\n",
    "          \"action_selection\"  : e_decay_greedy_action_selection, # Action selection strategy\n",
    "          \"update_rule\"       : update_rule, # The update rule\n",
    "          \"epsilon\"           : 0.1,# The epsilon parameter. (try with 0.01 as well--will it get stuck to lockal minima?)\n",
    "          \"kappa\"             : 0.001, # Decay coefficient\n",
    "         }\n",
    "\n",
    "#Run the experiment and plot:\n",
    "rnd_estimates_e_decay_greedy, rnd_total_reward_e_decay_greedy, rnd_mean_rewards_e_decay_greedy, rnd_regret_e_decay_greedy = procedure(inputs,horizon)\n",
    "plot_bandits(horizon, rnd_estimates_e_decay_greedy, rnd_total_reward_e_decay_greedy, rnd_mean_rewards_e_decay_greedy, rnd_regret_e_decay_greedy, q_star)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.6. Compare all:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see a comparison between the different initialization strategies for the e-Greedy with epsilon decay action selection strategy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_plot(regret_e_decay_greedy, min_regret_e_decay_greedy, avg_regret_e_decay_greedy,\\\n",
    "          max_regret_e_decay_greedy, rnd_regret_e_decay_greedy, mean_rewards_e_decay_greedy,\\\n",
    "          min_mean_rewards_e_decay_greedy, avg_mean_rewards_e_decay_greedy,\\\n",
    "          max_mean_rewards_e_decay_greedy, rnd_mean_rewards_e_decay_greedy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 8: The bandit algorithm with Upper Confidence Bound action selection and true average update (Experiment V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <div style=\"text-align: right\"><span style=\"color:red\">Maybe test out different c values? :)</span></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1. Initializing with zero values:\n",
    "What do you think the expected reward is? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Seed our random engines:\n",
    "rng2 = np.random.RandomState(1337)\n",
    "rng3 = np.random.RandomState(1337)\n",
    "\n",
    "#Initialize the experiment:\n",
    "inputs = {\"k\"                 : k, # Number of actions\n",
    "          \"horizon\"           : horizon, # Number of iterations\n",
    "          \"q_star\"            : q_star, # The true unknown rewards\n",
    "          \"initialization\"    : ZeroInitialization, # Initialization strategy\n",
    "          \"action_selection\"  : ucb_action_selection, # Action selection strategy\n",
    "          \"update_rule\"       : update_rule, # The update rule\n",
    "          \"c\"                 : 2. # The c parameter\n",
    "         }\n",
    "\n",
    "#Run the experiment and plot:\n",
    "estimates_ucb, total_reward_ucb, mean_rewards_ucb, regret_ucb = procedure(inputs,horizon)\n",
    "plot_bandits(horizon, estimates_ucb, total_reward_ucb, mean_rewards_ucb, regret_ucb, q_star)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2. Initializing with Pessimistic:\n",
    "What do you think the expected reward is? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seed our random engines:\n",
    "rng2 = np.random.RandomState(1337)\n",
    "rng3 = np.random.RandomState(1337)\n",
    "\n",
    "#Initialize the experiment:\n",
    "inputs = {\"k\"                 : k, # Number of actions\n",
    "          \"horizon\"           : horizon, # Number of iterations\n",
    "          \"q_star\"            : q_star, # The true unknown rewards\n",
    "          \"initialization\"    : MinInitialization, # Initialization strategy\n",
    "          \"action_selection\"  : ucb_action_selection, # Action selection strategy\n",
    "          \"update_rule\"       : update_rule, # The update rule\n",
    "          \"c\"                 : 2. # The c parameter\n",
    "         }\n",
    "\n",
    "#Run the experiment and plot:\n",
    "min_estimates_ucb, min_total_reward_ucb, min_mean_rewards_ucb, min_regret_ucb = procedure(inputs,horizon)\n",
    "plot_bandits(horizon, min_estimates_ucb, min_total_reward_ucb, min_mean_rewards_ucb, min_regret_ucb, q_star)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3. Initializing with Mean:\n",
    "What do you think the expected reward is?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Seed our random engines:\n",
    "rng2 = np.random.RandomState(1337)\n",
    "rng3 = np.random.RandomState(1337)\n",
    "\n",
    "#Initialize the experiment:\n",
    "inputs = {\"k\"                 : k, # Number of actions\n",
    "          \"horizon\"           : horizon, # Number of iterations\n",
    "          \"q_star\"            : q_star, # The true unknown rewards\n",
    "          \"initialization\"    : MeanInitialization, # Initialization strategy\n",
    "          \"action_selection\"  : ucb_action_selection, # Action selection strategy\n",
    "          \"update_rule\"       : update_rule, # The update rule\n",
    "          \"c\"                 : 2. # The c parameter\n",
    "         }\n",
    "\n",
    "#Run the experiment and plot:\n",
    "avg_estimates_ucb, avg_total_reward_ucb, avg_mean_rewards_ucb, avg_regret_ucb = procedure(inputs,horizon)\n",
    "plot_bandits(horizon, avg_estimates_ucb, avg_total_reward_ucb, avg_mean_rewards_ucb, avg_regret_ucb, q_star)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4. Initializing with Optimistic:\n",
    "What do you think the expected reward is?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seed our random engines:\n",
    "rng2 = np.random.RandomState(1337)\n",
    "rng3 = np.random.RandomState(1337)\n",
    "\n",
    "#Initialize the experiment:\n",
    "inputs = {\"k\"                 : k, # Number of actions\n",
    "          \"horizon\"           : horizon, # Number of iterations\n",
    "          \"q_star\"            : q_star, # The true unknown rewards\n",
    "          \"initialization\"    : MaxInitialization, # Initialization strategy\n",
    "          \"action_selection\"  : ucb_action_selection, # Action selection strategy\n",
    "          \"update_rule\"       : update_rule, # The update rule\n",
    "          \"c\"                 : 2. # The c parameter\n",
    "         }\n",
    "\n",
    "#Run the experiment and plot:\n",
    "max_estimates_ucb, max_total_reward_ucb, max_mean_rewards_ucb, max_regret_ucb = procedure(inputs,horizon)\n",
    "plot_bandits(horizon, max_estimates_ucb, max_total_reward_ucb, max_mean_rewards_ucb, max_regret_ucb, q_star)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5. Initializing with Random\n",
    "What do you think the expected reward is?\n",
    "(we will use the average of 100 random initializations, so that the results are significant):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seed our random engines:\n",
    "rng2 = np.random.RandomState(1337)\n",
    "rng3 = np.random.RandomState(1337)\n",
    "\n",
    "#Initialize the experiment:\n",
    "inputs = {\"k\"                 : k, # Number of actions\n",
    "          \"horizon\"           : horizon, # Number of iterations\n",
    "          \"q_star\"            : q_star, # The true unknown rewards\n",
    "          \"initialization\"    : RandomInitialization, # Initialization strategy\n",
    "          \"action_selection\"  : ucb_action_selection, # Action selection strategy\n",
    "          \"update_rule\"       : update_rule, # The update rule\n",
    "          \"c\"                 : 2. # The c parameter\n",
    "         }\n",
    "\n",
    "#Run the experiment and plot:\n",
    "rnd_estimates_ucb, rnd_total_reward_ucb, rnd_mean_rewards_ucb, rnd_regret_ucb = procedure(inputs,horizon)\n",
    "plot_bandits(horizon, rnd_estimates_ucb, rnd_total_reward_ucb, rnd_mean_rewards_ucb, rnd_regret_ucb, q_star)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.6. Compare all:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see a comparison between the different initialization strategies for the ucb action selection strategy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_plot(regret_ucb, min_regret_ucb, avg_regret_ucb, max_regret_ucb, rnd_regret_ucb, \\\n",
    "          mean_rewards_ucb, min_mean_rewards_ucb, avg_mean_rewards_ucb, max_mean_rewards_ucb, \\\n",
    "          rnd_mean_rewards_ucb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 9: The bandit algorithm with Softmax action selection and true average update (Experiment VI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <div style=\"text-align: right\"><span style=\"color:red\">Maybe test out different τ values? What happens very close to 0 and with very high τ values? :)</span></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1. Initializing with zero values:\n",
    "What do you think the expected reward is? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seed our random engines:\n",
    "rng2 = np.random.RandomState(1337)\n",
    "rng3 = np.random.RandomState(1337)\n",
    "\n",
    "#Initialize the experiment:\n",
    "inputs = {\"k\"                 : k, # Number of actions\n",
    "          \"horizon\"           : horizon, # Number of iterations\n",
    "          \"q_star\"            : q_star, # The true unknown rewards\n",
    "          \"initialization\"    : ZeroInitialization, # Initialization strategy\n",
    "          \"action_selection\"  : softmax_action_selection, # Action selection strategy\n",
    "          \"update_rule\"       : update_rule, # The update rule\n",
    "          \"tau\"               : 0.3 # The tau parameter\n",
    "         }\n",
    "\n",
    "#Run the experiment and plot:\n",
    "estimates_sm, total_reward_sm, mean_rewards_sm, regret_sm = procedure(inputs,horizon)\n",
    "plot_bandits(horizon, estimates_sm, total_reward_sm, mean_rewards_sm, regret_sm, q_star)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2. Initializing with Pessimistic:\n",
    "What do you think the expected reward is? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seed our random engines:\n",
    "rng2 = np.random.RandomState(1337)\n",
    "rng3 = np.random.RandomState(1337)\n",
    "\n",
    "#Initialize the experiment:\n",
    "inputs = {\"k\"                 : k, # Number of actions\n",
    "          \"horizon\"           : horizon, # Number of iterations\n",
    "          \"q_star\"            : q_star, # The true unknown rewards\n",
    "          \"initialization\"    : MinInitialization, # Initialization strategy\n",
    "          \"action_selection\"  : softmax_action_selection, # Action selection strategy\n",
    "          \"update_rule\"       : update_rule, # The update rule\n",
    "          \"tau\"               : 0.3 # The tau parameter\n",
    "         }\n",
    "\n",
    "#Run the experiment and plot:\n",
    "min_estimates_sm, min_total_reward_sm, min_mean_rewards_sm, min_regret_sm = procedure(inputs,horizon)\n",
    "plot_bandits(horizon, min_estimates_sm, min_total_reward_sm, min_mean_rewards_sm, min_regret_sm, q_star)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3. Initializing with Mean:\n",
    "What do you think the expected reward is?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seed our random engines:\n",
    "rng2 = np.random.RandomState(1337)\n",
    "rng3 = np.random.RandomState(1337)\n",
    "\n",
    "#Initialize the experiment:\n",
    "inputs = {\"k\"                 : k, # Number of actions\n",
    "          \"horizon\"           : horizon, # Number of iterations\n",
    "          \"q_star\"            : q_star, # The true unknown rewards\n",
    "          \"initialization\"    : MeanInitialization, # Initialization strategy\n",
    "          \"action_selection\"  : softmax_action_selection, # Action selection strategy\n",
    "          \"update_rule\"       : update_rule, # The update rule\n",
    "          \"tau\"               : 0.3 # The tau parameter\n",
    "         }\n",
    "\n",
    "#Run the experiment and plot:\n",
    "avg_estimates_sm, avg_total_reward_sm, avg_mean_rewards_sm, avg_regret_sm = procedure(inputs,horizon)\n",
    "plot_bandits(horizon, avg_estimates_sm, avg_total_reward_sm, avg_mean_rewards_sm, avg_regret_sm, q_star)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.4. Initializing with Optimistic:\n",
    "What do you think the expected reward is?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seed our random engines:\n",
    "rng2 = np.random.RandomState(1337)\n",
    "rng3 = np.random.RandomState(1337)\n",
    "\n",
    "#Initialize the experiment:\n",
    "inputs = {\"k\"                 : k, # Number of actions\n",
    "          \"horizon\"           : horizon, # Number of iterations\n",
    "          \"q_star\"            : q_star, # The true unknown rewards\n",
    "          \"initialization\"    : MaxInitialization, # Initialization strategy\n",
    "          \"action_selection\"  : softmax_action_selection, # Action selection strategy\n",
    "          \"update_rule\"       : update_rule, # The update rule\n",
    "          \"tau\"               : 0.3 # The tau parameter\n",
    "         }\n",
    "\n",
    "#Run the experiment and plot:\n",
    "max_estimates_sm, max_total_reward_sm, max_mean_rewards_sm, max_regret_sm = procedure(inputs,horizon)\n",
    "plot_bandits(horizon, max_estimates_sm, max_total_reward_sm, max_mean_rewards_sm, max_regret_sm, q_star)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.5. Initializing with Random\n",
    "What do you think the expected reward is?\n",
    "(we will use the average of 100 random initializations, so that the results are significant):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seed our random engines:\n",
    "rng2 = np.random.RandomState(1337)\n",
    "rng3 = np.random.RandomState(1337)\n",
    "\n",
    "#Initialize the experiment:\n",
    "inputs = {\"k\"                 : k, # Number of actions\n",
    "          \"horizon\"           : horizon, # Number of iterations\n",
    "          \"q_star\"            : q_star, # The true unknown rewards\n",
    "          \"initialization\"    : RandomInitialization, # Initialization strategy\n",
    "          \"action_selection\"  : softmax_action_selection, # Action selection strategy\n",
    "          \"update_rule\"       : update_rule, # The update rule\n",
    "          \"tau\"               : 0.3 # The tau parameter\n",
    "         }\n",
    "\n",
    "#Run the experiment and plot:\n",
    "rnd_estimates_sm, rnd_total_reward_sm, rnd_mean_rewards_sm, rnd_regret_sm = procedure(inputs,horizon)\n",
    "plot_bandits(horizon, rnd_estimates_sm, rnd_total_reward_sm, rnd_mean_rewards_sm, rnd_regret_sm, q_star)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.6. Compare all:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see a comparison between the different initialization strategies for the softmax action selection strategy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_plot(regret_sm, min_regret_sm, avg_regret_sm, max_regret_sm, rnd_regret_sm, \\\n",
    "          mean_rewards_sm, min_mean_rewards_sm, avg_mean_rewards_sm, max_mean_rewards_sm, \\\n",
    "          rnd_mean_rewards_sm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 10: Comparing Bandit Algorithms\n",
    "Compare the bandit algorithms through plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot the results of all the algorithms ###\n",
    "##############################################\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(np.linspace(0,horizon-1,horizon), np.cumsum(regret_rand), 'b-')\n",
    "ax.plot(np.linspace(0,horizon-1,horizon), np.cumsum(regret_greedy), 'g-')\n",
    "ax.plot(np.linspace(0,horizon-1,horizon), np.cumsum(regret_e_greedy), 'r-')\n",
    "ax.plot(np.linspace(0,horizon-1,horizon), np.cumsum(regret_e_decay_greedy), 'c-')\n",
    "ax.plot(np.linspace(0,horizon-1,horizon), np.cumsum(regret_ucb), 'm-')\n",
    "ax.plot(np.linspace(0,horizon-1,horizon), np.cumsum(regret_sm), 'y-')\n",
    "ax.grid()\n",
    "ax.set(xlabel='Time Steps', ylabel='Total Regret',\n",
    "       title='Regret Curve')\n",
    "plt.legend(['Random','Greedy','e-Greedy','e-Greedy with e decay','UCB','Softmax'])\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(np.linspace(0,horizon-1,horizon), mean_rewards_rand, 'b-')\n",
    "ax.plot(np.linspace(0,horizon-1,horizon), mean_rewards_greedy, 'g-')\n",
    "ax.plot(np.linspace(0,horizon-1,horizon), mean_rewards_e_greedy, 'r-')\n",
    "ax.plot(np.linspace(0,horizon-1,horizon), mean_rewards_e_decay_greedy, 'c-')\n",
    "ax.plot(np.linspace(0,horizon-1,horizon), mean_rewards_ucb, 'm-')\n",
    "ax.plot(np.linspace(0,horizon-1,horizon), mean_rewards_sm, 'y-')\n",
    "ax.grid()\n",
    "ax.set(xlabel='Time Steps', ylabel='Average Reward',\n",
    "       title='Average Reward Curve')\n",
    "plt.legend(['Random','Greedy','e-Greedy','e-Greedy with e decay','UCB','Softmax'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 11: Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.1. Exercise 1: Implement the gradient bandits action selection and update rule methods yourself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the gradient bandits with zeros initialization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Will gradient bandits have a different result using other initializations?\n",
    "Run the gradient bandits using different initializations to find out if you were correct:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.2. Exercise 2: Implement a bandit of your own liking!\n",
    "(A hybrid bandit maybe? Play with constant step size? Compare different greedy ones? Different gradient-based ones?)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
